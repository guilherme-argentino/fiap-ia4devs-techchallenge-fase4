{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guilherme-argentino/fiap-ia4devs-techchallenge-fase4/blob/main/Fase4_Tech_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reconhecimento Facial\n",
        "\n",
        "Desafio numero 4: analisando rostos, sentimentos e atividades."
      ],
      "metadata": {
        "id": "U3AkO68SHJBJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalar Dependências"
      ],
      "metadata": {
        "id": "SRCZdI44HYof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar dependências necessárias\n",
        "!pip install google-generativeai opencv-python opencv-python-headless matplotlib gdown deepface mediapipe fpdf --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNVuz7o3lPGx",
        "outputId": "5cfaa837-6be9-401d-e040-5b187532c124"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importar bibliotecas"
      ],
      "metadata": {
        "id": "Zilt9SzwHdwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar bibliotecas\n",
        "import gdown\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import google.generativeai as gemini\n",
        "import time\n",
        "import base64\n",
        "from deepface import DeepFace\n",
        "import dlib\n",
        "import pandas as pd\n",
        "import mediapipe as mp\n",
        "from fpdf import FPDF"
      ],
      "metadata": {
        "id": "PeYCLMW7lSNg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download do vídeo"
      ],
      "metadata": {
        "id": "KkmfanhXHkgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baixar o vídeo diretamente do Google Drive\n",
        "VIDEO_ID = \"1B5PbZdUDi-r7Ac7BK3a3WdNppfQqM_Ne\"  # ID do arquivo no Google Drive\n",
        "VIDEO_PATH = \"/content/video.mp4\"\n",
        "gdown.download(f\"https://drive.google.com/uc?id={VIDEO_ID}\", VIDEO_PATH, quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "IG8d9DN7lbcg",
        "outputId": "1a25d6c7-354b-45fd-ad74-c0a2bd961be4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1B5PbZdUDi-r7Ac7BK3a3WdNppfQqM_Ne\n",
            "To: /content/video.mp4\n",
            "100%|██████████| 38.3M/38.3M [00:00<00:00, 78.7MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/video.mp4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funções auxiliares"
      ],
      "metadata": {
        "id": "KCumIN9UHpt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuração do Mediapipe para detecção de atividades\n",
        "mp_pose = mp.solutions.pose\n",
        "pose = mp_pose.Pose()\n",
        "\n",
        "def analisar_emocao(frame):\n",
        "    try:\n",
        "        analysis = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)\n",
        "        return analysis[0]['dominant_emotion']\n",
        "    except:\n",
        "        return \"desconhecido\"\n",
        "\n",
        "def detectar_atividade(frame):\n",
        "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    results = pose.process(rgb_frame)\n",
        "    return \"Movimento detectado\" if results.pose_landmarks else \"Sem atividade\"\n",
        "\n",
        "def processar_video(video_path, output_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "    dados = []\n",
        "    frame_count = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_count += 1\n",
        "        frame, num_faces = detectar_faces(frame, detector)\n",
        "        emocao = analisar_emocao(frame)\n",
        "        atividade = detectar_atividade(frame)\n",
        "        dados.append([frame_count, num_faces, emocao, atividade])\n",
        "        out.write(frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    return dados\n",
        "\n",
        "def gerar_relatorio(dados, relatorio_path):\n",
        "    df = pd.DataFrame(dados, columns=['Frame', 'Rostos', 'Expressão', 'Atividade'])\n",
        "    pdf = FPDF()\n",
        "    pdf.add_page()\n",
        "    pdf.set_font(\"Arial\", size=12)\n",
        "    pdf.cell(200, 10, \"Relatório de Análise de Vídeo\", ln=True, align='C')\n",
        "    pdf.cell(200, 10, f\"Total de Frames: {len(df)}\", ln=True, align='L')\n",
        "    pdf.cell(200, 10, f\"Rostos Detectados: {df['Rostos'].sum()}\", ln=True, align='L')\n",
        "    pdf.output(relatorio_path)"
      ],
      "metadata": {
        "id": "8aufSB1am74Z"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fluxo principal\n",
        "\n",
        "Execução das rotinas de processamento, usando as bibliotecas OpenCV, DLib, Mediapipe e Deepface. No final, gera um PDF."
      ],
      "metadata": {
        "id": "kwuPCrGCHvBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detectar_faces(frame, detector):\n",
        "    # Convert to RGB and ensure it's a contiguous array with the correct dtype\n",
        "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    rgb_frame = np.ascontiguousarray(rgb_frame, dtype=np.uint8)\n",
        "\n",
        "    # Detect faces using the RGB image\n",
        "    faces = detector(rgb_frame)\n",
        "\n",
        "    # Draw bounding boxes on detected faces (using the original frame)\n",
        "    for face in faces:\n",
        "        x, y, w, h = face.left(), face.top(), face.width(), face.height()\n",
        "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "    return frame, len(faces)\n",
        "\n",
        "# Processar vídeo com OpenCV, DLib, Mediapipe, Deepface\n",
        "def main():\n",
        "    video_path = \"/content/video.mp4\"\n",
        "    output_path = \"results/output_video.avi\"\n",
        "    relatorio_path = \"results/relatorio.pdf\"\n",
        "    dados = processar_video(video_path, output_path)\n",
        "    gerar_relatorio(dados, relatorio_path)\n",
        "    print(\"Processamento concluído! Relatório gerado.\")\n",
        "\n",
        "# Executar o pipeline\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "PP2tC1c5GpJQ",
        "outputId": "5ee3015c-88de-4c01-9d06-8e76a774bd71"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Unsupported image type, must be 8bit gray or RGB image.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-a51d428de5ce>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Executar o pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-a51d428de5ce>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0moutput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"results/output_video.avi\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mrelatorio_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"results/relatorio.pdf\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mdados\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessar_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mgerar_relatorio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdados\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelatorio_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Processamento concluído! Relatório gerado.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-d0ac83828f84>\u001b[0m in \u001b[0;36mprocessar_video\u001b[0;34m(video_path, output_path)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mframe_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_faces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetectar_faces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0memocao\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalisar_emocao\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0matividade\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetectar_atividade\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-a51d428de5ce>\u001b[0m in \u001b[0;36mdetectar_faces\u001b[0;34m(frame, detector)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Detect faces using the RGB image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Draw bounding boxes on detected faces (using the original frame)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Unsupported image type, must be 8bit gray or RGB image."
          ]
        }
      ]
    }
  ]
}